{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46281df-667d-41ef-840a-680ac86449d9",
   "metadata": {},
   "source": [
    "<img src='data:image/svg+xml;base64,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' align='left' style='margin:30px' />\n",
    "\n",
    "<div style='background-color: #567890; color:white; padding:15px;'>\n",
    "    \n",
    "### Проект по спринту «Машинное обучение для текстов»: <br>классификация комментариев с BERT\n",
    "\n",
    "<br><br>\n",
    "Интернет-магазин N запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "<br><br>\n",
    "#### Цели работы\n",
    "\n",
    "Обучить модель классифицировать комментарии на позитивные и негативные. Метрика качества F1 должна быть не меньше 0.75. \n",
    "<br><br>\n",
    "\n",
    "#### План работы\n",
    "\n",
    "1. Предобработка данных\n",
    "2. Векторизация текстов\n",
    "3. Исправление дисбаланса данных\n",
    "4. Проверка датасета на лог. регрессии\n",
    "5. Проверка на градиентном бустинге\n",
    "6. Выводы по классическим моделям для текстов\n",
    "7. Подготовка данных для модели BERT\n",
    "8. Тренировочный и валидационный цикл модели BERT\n",
    "10. Результаты по модели BERT\n",
    "11. Проверка датасета на предобученной модели RoBERTa\n",
    "12. Результаты по модели RoBERTa\n",
    "13. Выводы по проекту\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab016cf-7cd2-4963-854c-89c55597270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing standart libs\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import time as tm\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "#importing additional libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# models and stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# neiro-things\n",
    "import torch\n",
    "import transformers\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdd0e8-b307-4fb3-ad42-1c4521be96e1",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid darkred; padding: 20px\">\n",
    "    Примечание: в такой конфигурации проект не будет считать данные моделями Берт и Роберта, но будет их инициализировать (в результате Робертой можно проверить любой текст в конце проекта). Если захочется проверить работу кода обучения\\валидации, нужно указать False в параметрах SKIP_BERT, SKIP_ROBERTA\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b78440-5451-4be9-aaa8-ac9d2e11d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "\n",
    "DATA_PATH = './datasets/' # the datasets path\n",
    "DATA_PATH_ALT = '/datasets/' # alternative path for reviewer\n",
    "DATA_FILENAME = 'toxic_comments.csv'\n",
    "RANDOM_STATE = 2042 # a random number for data shuffling to be constant\n",
    "TEST_SIZE = 0.15\n",
    "MIN_METRIC = 0.75 # min value of quality metric we need to achieve\n",
    "RESEARCH_MODE = True # if True, we process only a small part of a dataset to speed up all the calcs\n",
    "DATASET_SPLIT = 0.05 # which part of dataset we're processing in RESEARCH_MODE\n",
    "SKIP_BERT = True # do we need to train&check BERT model?\n",
    "SKIP_ROBERTA = True # do we need to skip cheking quality on RoBERTa model?\n",
    "EPS = 1e-9 # a small number that helps to prevent 0_division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7735d48-ff3f-4446-855d-3b5249383519",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pre-settings\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# switching off the warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255b28f-53e3-4a40-9a19-d829081e8138",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d950a23-f241-4309-ad98-643201cbbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets loading\n",
    "\n",
    "try:\n",
    "    if os.path.exists(DATA_PATH + DATA_FILENAME):\n",
    "        path = DATA_PATH\n",
    "    else:\n",
    "        path = DATA_PATH_ALT\n",
    "    tw = pd.read_csv(path + DATA_FILENAME)\n",
    "except:\n",
    "    print(f'Ошибка в загрузке датасетов! Проверьте, пожалуйста, что есть каталог {DATA_PATH} или {DATA_PATH_ALT}, в котором лежит файл {DATA_FILENAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a2bb9-048a-4821-bf02-7b57833ea5b6",
   "metadata": {},
   "source": [
    "Выведем начало датасета и общую информацию о нём."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa9f493-e292-470a-ab40-6645be088387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "display(tw.head())\n",
    "print(tw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb6db7-97e1-4a6c-9395-9455b68905fe",
   "metadata": {},
   "source": [
    "Большие датасеты приводят к значительным временным затратам при обсчитывании проекта. Поэтому во время разработки проекта мы будем использовать \"исследовательский режим\" - вместо полного датасета возьмём лишь его малую часть (в данном случае, 5%). Убедившись в работоспособности всего кода, мы проведём расчёты на полной выборке.<br>Отключить режим можно в разделе установки констант, задав значение <code>RESEARCH_MODE = False</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9934fe3f-697c-4457-b18f-9a777097e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE) # important one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0e5b39-af98-4ad3-a7f4-8320596c08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESEARCH_MODE:\n",
    "    idx_sample = np.random.randint(0, len(tw), size=round(len(tw)*DATASET_SPLIT))\n",
    "    tw = tw.iloc[idx_sample]\n",
    "else:\n",
    "    DATASET_SPLIT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3747e0bf-be35-4bf9-982b-96bebc25a5b0",
   "metadata": {},
   "source": [
    "> Важный момент для проверки: ниже выводятся числа, равны ли они 8047, 60175, 52784, 48837, 35695 ?\n",
    "\n",
    "Если нет, нужно перезапускать расчёт эмбеддингов, а не считывать их из заранее рассчитанного файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c001ba-e0c0-4c0c-b505-04833d4c0b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8047, 60175, 52784, 48837, 35695])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053a72b-0cf0-4a9a-b696-0c7a2ba03dd2",
   "metadata": {},
   "source": [
    "Дополнительная проверка - в каком режиме мы находимся? Размер полного датасета = 159292 значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1ffbc6-16ab-4413-8815-ef6bcaae7ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7965 entries, 8047 to 155309\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  7965 non-null   int64 \n",
      " 1   text        7965 non-null   object\n",
      " 2   toxic       7965 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 248.9+ KB\n"
     ]
    }
   ],
   "source": [
    "tw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60731af-270d-4c82-9c1c-ba0af91c9537",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701deffc-a6cd-4fa2-b51d-1eb131c9a8be",
   "metadata": {},
   "source": [
    "#### Приведение типов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc899d97-ca90-4aa3-b659-cbc9fbe0f3c5",
   "metadata": {},
   "source": [
    "Колонка <code>toxic</code> имеет излишне \"увесистый\" тип данных для значений 0 и 1. Изменим тип для оптимизации размера датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f156a325-d382-4f72-951c-9132846bd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw['toxic'] = tw['toxic'].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "647dc040-56d5-4e64-b629-3b1ad96cde62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7965 entries, 8047 to 155309\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  7965 non-null   int64 \n",
      " 1   text        7965 non-null   object\n",
      " 2   toxic       7965 non-null   uint8 \n",
      "dtypes: int64(1), object(1), uint8(1)\n",
      "memory usage: 194.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584e518-d4e6-4780-853d-4ead8bfb0934",
   "metadata": {},
   "source": [
    "#### Очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a94ffa-1656-419b-949f-7fca50d95626",
   "metadata": {},
   "source": [
    "Прежде всего, обратим внимание на колонку <code>Unnamed: 0</code> - она представляется лишней, индекс у нас есть. Удалим её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679482cf-5074-4927-a2a8-c35d0e8aaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = tw.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf88c1-b7ce-4563-9b46-cd7d4baa98d9",
   "metadata": {},
   "source": [
    "В комментариях видим большое количество символов перевода строки <code>\\n</code>. Нам они тоже не понадобятся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fcd3b15-f697-480f-a65f-e7f441d9937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw['text'] = tw['text'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ac831-3256-4902-86d0-e8d62a2f890c",
   "metadata": {},
   "source": [
    "Перед токенизацией удалим также стоп-слова и ненужные символы - для задачи классификации можно обойтись без цифр и пунктуации.<br>\n",
    "Для этого напишем отдельные функции, а затем применим их к каждой ячейке столбца <code>text</code>.<br>Лемматизация (а также перевод в нижний регистр) нам не нужна, поскольку мы будем использовать продвинутую модель RuBert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cacab3c-1881-4141-a878-eb76c5d9f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords loading from NLTK\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk_stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "106cfab4-6411-4d32-80e2-22e3777d37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removing(text: str, stopwords: list) -> str:\n",
    "    '''\n",
    "    -= a function that removes all the stopwords from given text =-\n",
    "    '''\n",
    "    filtered_words = [word for word in text.split() if word.lower() not in stopwords]\n",
    "    return (\" \".join(filtered_words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "985508c8-c66b-4c66-a17d-b581db11c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clearing(text: str) -> str:\n",
    "    '''\n",
    "    -= a function that removes all the symbols from given text exclude english and cyrillic letters,\n",
    "    making text lowercased =-\n",
    "    '''\n",
    "    # getting rid of dates like '21:51, January 11, 2016 (UTC)'\n",
    "    re_timedate = r\"\\d\\d:\\d\\d..[A-Z]\\w+\\s\\d\\d..\\d\\d\\d\\d?(.\\(\\w+\\))?\"\n",
    "    re_timedate_day_first = r\"\\d\\d:\\d\\d..\\d\\d.[A-Z]\\w+.\\d\\d\\d\\d.?(\\(\\w[A-Z]+\\))?\"\n",
    "     \n",
    "    text = re.compile(re_timedate).sub(' ', text)\n",
    "    text = re.compile(re_timedate_day_first).sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ]', ' ', text).lower()\n",
    "    return \" \".join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53064dfe-d184-4833-95c6-1076c3594caf",
   "metadata": {},
   "source": [
    "Последовательно убираем стоп-слова (в актуальной версии проекта может быть disabled, потому что большие модели в этом не нуждаются) и очищаем текст комментариев от лишнего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "759585d5-8597-4cb5-ae03-4dcefcfc1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tw['text'] = tw['text'].apply(lambda x: stopwords_removing(x, stopwords=stopwords)).apply(text_clearing)\n",
    "\n",
    "tw['text'] = tw['text'].apply(text_clearing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c5842-4a66-4c1f-b86d-3216615340e2",
   "metadata": {},
   "source": [
    "Всё ли идёт хорошо? Выведем начало датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd5d614c-2839-4ae4-baf4-363581ef48c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>this post placed here grazon who does not sign his her postings on wikipedia i don t understand what you mean political beliefs are you implying that i am taking the opposite side of this argument because harding is a republican ergo i feel the need to protect him i m sticking up for the man because i have looked at primary resources and know what they say and what they do not contain which is any inkling of klan involvement what i can t figure out is why is it so important for you include harding in the klan that you would base you whole argument on what one book claims</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60175</th>\n",
       "      <td>notability tags please do not remove article tags without explanation and without addressing issues let me know if you have any questions as to how to do this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52784</th>\n",
       "      <td>love this page i ve just made several additions to farragher s boxing record on boxrec com another youngstown boxer who deserves a wiki page is squirrel finnerty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>you might want to have it more on his occultism he was heavy into seances and the such there are many of his plays that deal with the matter his understanding of time the soul and immortality are likewise interesting it would probably be best to come up with a separate page first and then summarize that page in a section william butler yeats s occultism something along that line</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35695</th>\n",
       "      <td>felix lupa deleted page my name is vered sadot and i m a brand new user on you have deleted page i edited felix lupa a article about a real person which does not indicate the importance or significance of the subject if possible pls indicate what made you think that the subject wasn t significant or important the real person is a great street photographer in israel and worldwide such as other israeli photographers on list and more i would like to create a new page and do it right so i realy need your opinion x a lot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "8047   this post placed here grazon who does not sign his her postings on wikipedia i don t understand what you mean political beliefs are you implying that i am taking the opposite side of this argument because harding is a republican ergo i feel the need to protect him i m sticking up for the man because i have looked at primary resources and know what they say and what they do not contain which is any inkling of klan involvement what i can t figure out is why is it so important for you include harding in the klan that you would base you whole argument on what one book claims   \n",
       "60175                                                                                                                                                                                                                                                                                                                                                                                                                                     notability tags please do not remove article tags without explanation and without addressing issues let me know if you have any questions as to how to do this   \n",
       "52784                                                                                                                                                                                                                                                                                                                                                                                                                                  love this page i ve just made several additions to farragher s boxing record on boxrec com another youngstown boxer who deserves a wiki page is squirrel finnerty   \n",
       "48837                                                                                                                                                                                                      you might want to have it more on his occultism he was heavy into seances and the such there are many of his plays that deal with the matter his understanding of time the soul and immortality are likewise interesting it would probably be best to come up with a separate page first and then summarize that page in a section william butler yeats s occultism something along that line   \n",
       "35695                                                          felix lupa deleted page my name is vered sadot and i m a brand new user on you have deleted page i edited felix lupa a article about a real person which does not indicate the importance or significance of the subject if possible pls indicate what made you think that the subject wasn t significant or important the real person is a great street photographer in israel and worldwide such as other israeli photographers on list and more i would like to create a new page and do it right so i realy need your opinion x a lot   \n",
       "\n",
       "       toxic  \n",
       "8047       0  \n",
       "60175      0  \n",
       "52784      0  \n",
       "48837      0  \n",
       "35695      0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e1cc2-56e0-4b94-ac3d-d84075d4b5d8",
   "metadata": {},
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86d461-1e04-4c97-988a-e0d7cc9b89a0",
   "metadata": {},
   "source": [
    "С помощью библиотеки transformers загрузим токенизатор для модели Bert.\n",
    "\n",
    "Если мы уже проводили процедуры токенизации, добавления маски внимания и паддинга при текущем параметре DATASET_SPLIT, считаем полученный файл с результатами, что существенно ускорит исследовательский процесс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c974007e-cc31-469a-aaf5-c28c00307c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_masked_padded = False\n",
    "\n",
    "if os.path.exists('tokenized_masked_padded_x'+str(DATASET_SPLIT)+'.npy'):\n",
    "    tokenized_masked_padded = True\n",
    "\n",
    "    # saving to .csv messes with lists of nums, so we use numpy -> pd.DF\n",
    "    tw = np.load('tokenized_masked_padded_x'+str(DATASET_SPLIT)+'.npy', allow_pickle=True)\n",
    "    tw = pd.DataFrame(tw)\n",
    "    tw.rename(columns={0:'text', 1: 'toxic', 2:'tokens', 3:'mask'}, inplace=True)\n",
    "    tw['toxic'] = tw['toxic'].astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099712f-b5bf-4be5-bc07-7ef958d653cf",
   "metadata": {},
   "source": [
    "Токенизируем все наши тексты. Запишем токены в отдельную колонку.<br>Процедура на полном датасете займёт примерно 2,5 минуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7d5d2ce-0133-42ce-abaa-94bb36e7bfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not(tokenized_masked_padded):\n",
    "    \n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    tw['tokens'] = tw['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "591f78e1-2650-470e-8939-2f46d60f99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7825 entries, 0 to 7824\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7825 non-null   object\n",
      " 1   toxic   7825 non-null   uint8 \n",
      " 2   tokens  7825 non-null   object\n",
      " 3   mask    7825 non-null   object\n",
      "dtypes: object(3), uint8(1)\n",
      "memory usage: 191.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee114038-b7e6-4e9d-b21f-e5bd9f5b77ad",
   "metadata": {},
   "source": [
    "#### Паддинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83a601-043d-4b42-a73e-efb2f8db12c6",
   "metadata": {},
   "source": [
    "Длины всех токенизированных текстов должны быть равны, для того чтобы модель смогла работать. Поэтому осуществим процедуру padding - заполнение нулями недостающих до макс. значения позиций.<br>\n",
    "Но сперва это максимальное значение (максимальная длина комментария в датасете) надо найти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f988ff12-97d5-4255-a0b2-c88d721c56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(tokenized_masked_padded):\n",
    "    \n",
    "    max_len = 0\n",
    "    for i in range(tw.shape[0]):\n",
    "        if len(tw.iloc[i]['tokens']) > max_len:\n",
    "            max_len = len(tw.iloc[i]['tokens'])\n",
    "            max_idx = i\n",
    "    \n",
    "    print(f'Максимальная длина комментария = {max_len} токенов')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc019a24-5be2-4980-9b10-02ddb50a91ff",
   "metadata": {},
   "source": [
    "Ради интереса посмотрим, что же это за пространное сочинение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ebdb4ac-7cdc-4c9c-983b-4f77c7b32d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(tokenized_masked_padded):\n",
    "    display(tw.iloc[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316c14c-cfa6-4fc2-877f-32fd36556bb9",
   "metadata": {},
   "source": [
    "Очень информативно, nice. ( ͡° ͜ʖ ͡°)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501921e-1077-4ee8-bd55-2d795b5a5501",
   "metadata": {},
   "source": [
    "#### <i>Взгляд из будущего.</i>\n",
    "При использовании модели BERT для векторизации выяснилось, что она принимает на вход не больше 512-и токенов за раз. Что можно с этим сделать?\n",
    "- Разбивание входного вектора на отдельные части < 512 токенов кажется не рабочим методом для задачи классификации, ведь часть комментария, определяющая его токсичность, может оказаться в одном таком разбиении, а при этом все части разбиения получат одну оценку - оценку изначального, целого комментария (другой оценки у нас нет).\n",
    "- Можно усекать длинные комментарии до размера 512 (прямо в токенизаторе для этого есть параметры <code>truncation</code> и <code>max_length</code>) токенов. Против этого аналогичные аргументы - что если токсичная часть находится за пределами 512-го токена? Модель обучится не тому.\n",
    "- Длинные комментарии можно выбросить из датасета. Кажется единственно подходящим вариантом, который не снизит качество модели - но только если не сильно уменьшит размер датасета.\n",
    "- Применение более продвинутой модели, допускающей большее количество входных признаков. Но здесь могут быть проблемы при валидации проекта ревьюером на окружении Я.Практикума, вплоть до невозможности проверки.\n",
    "\n",
    "\n",
    "Попробуем для начала подсчитать количество комментариев, которые в токенизированном виде длиннее 512-и токенов. Сделаем это для полного датасета; если \"текстовый\" вывод отличается от результата исполнения кода - вы в \"исследовательском\" режиме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83dfc8f4-a4b4-4edb-b844-ed584b4506f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 996 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not(tokenized_masked_padded):\n",
    "    \n",
    "    max_len = 512\n",
    "    cnt = 0\n",
    "    idx_long_comments = []\n",
    "    \n",
    "    for i in range(len(tw['tokens'])):\n",
    "        if len(tw.iloc[i]['tokens']) > max_len:\n",
    "            cnt += 1\n",
    "            idx_long_comments.append(tw.iloc[i].name)    \n",
    "        \n",
    "    print(f'Количество комментариев, которые длиннее {max_len} токенов = {cnt}')\n",
    "    print(f'Процент \"длинных\" комментариев в датасете: {cnt/tw.shape[0]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a35d3-57ca-4bc2-acc8-dcb1a15cbe57",
   "metadata": {},
   "source": [
    "> Количество \"длинных\" комментариев = 2433, что составляет всего полтора процента от полного датасета. Их можно безболезненно удалить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18b4188d-2eb0-4335-a499-746115daf306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting 'long' comments\n",
    "if not(tokenized_masked_padded):\n",
    "    print('Размерность датасета до удаления =', tw.shape)\n",
    "    tw = tw.drop(idx_long_comments, axis=0)\n",
    "    print('Размерность датасета после удаления =', tw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2add429-d3c7-4a9f-b34b-fbf5ed43fc96",
   "metadata": {},
   "source": [
    "Возвращаемся к главной задаче этого этапа: превратим все токенизированные комментарии в объекты с одинаковой длиной (теперь эта длина не превысит 512 токенов), \"недостающие\" \"слова\" заполним нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81163838-ae7b-4ead-b909-92957f90ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(tokenized_masked_padded):\n",
    "    tw['tokens'] = tw['tokens'].apply(lambda x: np.array(x + [0]*(max_len - len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85179d-8099-499d-ae40-ec3be97c5fe2",
   "metadata": {},
   "source": [
    "Всё ли в порядке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae33727b-03ef-4924-a39d-beb65e6247df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this post placed here grazon who does not sign his her postings on wikipedia i don t understand what you mean political beliefs are you implying that i am taking the opposite side of this argument because harding is a republican ergo i feel the need to protect him i m sticking up for the man because i have looked at primary resources and know what they say and what they do not contain which is any inkling of klan involvement what i can t figure out is why is it so important for you include harding in the klan that you would base you whole argument on what one book claims</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2023, 2695, 2872, 2182, 26918, 2239, 2040, 2515, 2025, 3696, 2010, 2014, 14739, 2015, 2006, 16948, 1045, 2123, 1056, 3305, 2054, 2017, 2812, 2576, 9029, 2024, 2017, 20242, 2008, 1045, 2572, 2635, 1996, 4500, 2217, 1997, 2023, 6685, 2138, 15456, 2003, 1037, 3951, 9413, 3995, 1045, 2514, 1996, 2342, 2000, 4047, 2032, 1045, 1049, 13423, 2039, 2005, 1996, 2158, 2138, 1045, 2031, 2246, 2012, 3078, 4219, 1998, 2113, 2054, 2027, 2360, 1998, 2054, 2027, 2079, 2025, 5383, 2029, 2003, 2151, 10710, 2989, 1997, 26613, 6624, 2054, 1045, 2064, 1056, 3275, 2041, 2003, 2339, 2003, 2009, 2061, 2590, 2005, 2017, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notability tags please do not remove article tags without explanation and without addressing issues let me know if you have any questions as to how to do this</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2025, 8010, 22073, 3531, 2079, 2025, 6366, 3720, 22073, 2302, 7526, 1998, 2302, 12786, 3314, 2292, 2033, 2113, 2065, 2017, 2031, 2151, 3980, 2004, 2000, 2129, 2000, 2079, 2023, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love this page i ve just made several additions to farragher s boxing record on boxrec com another youngstown boxer who deserves a wiki page is squirrel finnerty</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2293, 2023, 3931, 1045, 2310, 2074, 2081, 2195, 13134, 2000, 2521, 29181, 5886, 1055, 8362, 2501, 2006, 3482, 2890, 2278, 4012, 2178, 2402, 13731, 10423, 2040, 17210, 1037, 15536, 3211, 3931, 2003, 18197, 9303, 15010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "0  this post placed here grazon who does not sign his her postings on wikipedia i don t understand what you mean political beliefs are you implying that i am taking the opposite side of this argument because harding is a republican ergo i feel the need to protect him i m sticking up for the man because i have looked at primary resources and know what they say and what they do not contain which is any inkling of klan involvement what i can t figure out is why is it so important for you include harding in the klan that you would base you whole argument on what one book claims   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                     notability tags please do not remove article tags without explanation and without addressing issues let me know if you have any questions as to how to do this   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                  love this page i ve just made several additions to farragher s boxing record on boxrec com another youngstown boxer who deserves a wiki page is squirrel finnerty   \n",
       "\n",
       "   toxic  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 tokens  \\\n",
       "0  [101, 2023, 2695, 2872, 2182, 26918, 2239, 2040, 2515, 2025, 3696, 2010, 2014, 14739, 2015, 2006, 16948, 1045, 2123, 1056, 3305, 2054, 2017, 2812, 2576, 9029, 2024, 2017, 20242, 2008, 1045, 2572, 2635, 1996, 4500, 2217, 1997, 2023, 6685, 2138, 15456, 2003, 1037, 3951, 9413, 3995, 1045, 2514, 1996, 2342, 2000, 4047, 2032, 1045, 1049, 13423, 2039, 2005, 1996, 2158, 2138, 1045, 2031, 2246, 2012, 3078, 4219, 1998, 2113, 2054, 2027, 2360, 1998, 2054, 2027, 2079, 2025, 5383, 2029, 2003, 2151, 10710, 2989, 1997, 26613, 6624, 2054, 1045, 2064, 1056, 3275, 2041, 2003, 2339, 2003, 2009, 2061, 2590, 2005, 2017, ...]   \n",
       "1                                                                                                                                                                                                                       [101, 2025, 8010, 22073, 3531, 2079, 2025, 6366, 3720, 22073, 2302, 7526, 1998, 2302, 12786, 3314, 2292, 2033, 2113, 2065, 2017, 2031, 2151, 3980, 2004, 2000, 2129, 2000, 2079, 2023, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]   \n",
       "2                                                                                                                                                                                                [101, 2293, 2023, 3931, 1045, 2310, 2074, 2081, 2195, 13134, 2000, 2521, 29181, 5886, 1055, 8362, 2501, 2006, 3482, 2890, 2278, 4012, 2178, 2402, 13731, 10423, 2040, 17210, 1037, 15536, 3211, 3931, 2003, 18197, 9303, 15010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83cbc8-daa7-41a9-bdf0-e91f0aa3d8a2",
   "metadata": {},
   "source": [
    "C помощью следующего кода можно удостовериться, что теперь любой токенизированный комментарий имеет фиксированную длину:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d1067db-1384-42f5-b6c1-91f919839a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Комментарий № 84, будучи токенизированным с паддингом, имеет длину 512 ток.\n"
     ]
    }
   ],
   "source": [
    "token_index = 84\n",
    "print(f\"Комментарий № {token_index}, будучи токенизированным с паддингом, имеет длину {len(tw['tokens'].iloc[token_index])} ток.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35e70d-eb1a-42a5-95d2-0d7db72fe5c9",
   "metadata": {},
   "source": [
    "#### Маскирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1086a-f621-4043-9718-16fc643c98c6",
   "metadata": {},
   "source": [
    "После того, как мы добавили нулевые значения в токенизированные комментарии, нам нужно задать для каждого такого комментария <i>маску</i> - вектор длиной ровно с токенизированный комментарий, который будет указывать, значащий ли токен на соответствующем месте (1), или не значащий (0).<br>\n",
    "Здесь нас в очередной раз выручит лямбда-функция, с помощью которой мы пробежимся по каждой ячейке колонки <code>tokens</code> и создадим в новой колонке <code>mask</code> np.array с единицей, если соответствующий токен не равен нулю - и нулём в обратном случае.<br><br>\n",
    "И на этом этапе мы уже можем записывать обработанный корпус в файл, чтобы не проделывать все процедуры заново при каждом запуске проекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "690c6d21-3d81-4301-9065-486f2a9555b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not(tokenized_masked_padded):\n",
    "    tw['mask'] = tw['tokens'].apply(lambda x: (x != 0).astype('int8'))\n",
    "\n",
    "    # saving our results to not calculate all of those from scratch\n",
    "    try:\n",
    "        np.save('tokenized_masked_padded_x'+str(DATASET_SPLIT)+'.npy', tw.to_numpy())\n",
    "    except:\n",
    "        print('Не получилось записать данные обработки текстов. Возможно, нет разрешения на запись в данном месте')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc22b2b-ebb2-4ff1-97ab-0f55d9d8fb02",
   "metadata": {},
   "source": [
    "Всё прошло успешно, и наконец-то у нас есть все данные для векторизации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e432be1f-68d7-4c33-8980-66865f8e1af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7825 entries, 0 to 7824\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7825 non-null   object\n",
      " 1   toxic   7825 non-null   uint8 \n",
      " 2   tokens  7825 non-null   object\n",
      " 3   mask    7825 non-null   object\n",
      "dtypes: object(3), uint8(1)\n",
      "memory usage: 191.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201c479-74ff-4ce4-856d-7d4181d89022",
   "metadata": {},
   "source": [
    "### Эмбеддинги (векторизация текстов через BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db6c51-f7d7-4c2c-9941-65291b91d8fc",
   "metadata": {},
   "source": [
    "#### Инициализация модели Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047cc84-27fd-44db-b32d-6aa3165e82fc",
   "metadata": {},
   "source": [
    "Пути до конфига модели и до самой модели, взятые из теории в Практикуме, никуда не привели при попытке работать с ними локально:\n",
    "<br><code> <Error> NoSuchKey\n",
    "<Message>The specified key does not exist.</Message>\n",
    "...\n",
    "</Error></code>Практикум такой Практикум (◣ _ ◢)<br><br>\n",
    "С другой стороны, RuBERT не предназначен для работы с английскими текстами, а у нас комментарии на english. Пришлось искать обходные пути - мы взяли конфиг и модель из Huggingface.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d30ad224-39f5-43ae-b4b9-f84cb85eae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_dict = transformers.BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "985febc5-9201-4f3b-aa5d-85475f4b2740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c23c7b2f-0027-44c9-8fd5-0bfc860f05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertModel.from_pretrained(pretrained_model_name_or_path='google-bert/bert-base-uncased', config=bert_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f567608-bc0e-46b0-873b-3f5dd07e2ae1",
   "metadata": {},
   "source": [
    "#### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f05b5-9d90-466c-b5b4-2893bbea962a",
   "metadata": {},
   "source": [
    "Для ускорения вычислений перенесём модель на GPU, если возможно (для этого нужно сперва установить драйвера CUDA).<br>\n",
    "В нашем случае обсчёт ускоряется в шесть раз!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11f95f24-ca07-415c-9264-f40c7e21d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "127c5b4a-d91c-48c5-afe6-cd147c9fc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какой тип устройства доступен (если cuda, то доступно вычисление на GPU):\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print('Какой тип устройства доступен (если cuda, то доступно вычисление на GPU):')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3096085-3d60-4f9b-9639-f0e2bcd0d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0028b655-9711-43fc-a395-a36b545d8581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 8.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# если мы ещё не записывали фичи, то просчитаем эмбеддинги\n",
    "if not(os.path.exists('features_x'+str(DATASET_SPLIT)+'.npy')):\n",
    "        \n",
    "    batch_size = 5\n",
    "    # пустой список для хранения эмбеддингов комментариев\n",
    "    embeddings = []\n",
    "    batch_num = tw.shape[0] // batch_size\n",
    "    for i in notebook.tqdm(range(batch_num)):\n",
    "        bottom = batch_size*i\n",
    "        batch = torch.LongTensor(tw[bottom:bottom+batch_size]['tokens'].reset_index(drop=True))\n",
    "        attention_mask_batch = torch.LongTensor(tw[bottom:bottom+batch_size]['mask'].reset_index(drop=True))\n",
    "        # данные должны быть на том же девайсе, где и модель, GPU или CPU\n",
    "        batch = batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "    \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "        \n",
    "        # printing some info every 20th batch since 'tqdm' doesn't work\n",
    "        if i % 25 == 0:\n",
    "            if i == 0:\n",
    "                print(f'Из {batch_num} обработан батч №', i, end=\" - \")\n",
    "            else:\n",
    "                print(i, end=' - ')\n",
    "    \n",
    "    # объединив эмбеддинги, получим набор фичей\n",
    "    features = np.concatenate(embeddings)\n",
    "    # запишем результат в файл, чтобы не ждать много минут в след. раз\n",
    "    try:\n",
    "        np.save('features_x'+str(DATASET_SPLIT), features)\n",
    "    except:\n",
    "        print('Не получилось записать данные обработки текстов. Возможно, нет разрешения на запись в данном месте')\n",
    "        \n",
    "# иначе считаем их из файла\n",
    "else:\n",
    "    features = np.load('features_x'+str(DATASET_SPLIT)+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73eb1b4c-67be-48ae-a757-3c8289138b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7825, 768)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b787e46-21d2-40f4-af05-1eebece3c1c9",
   "metadata": {},
   "source": [
    "> Иными словами, теперь каждый наш текстовый комментарий представляет собой набор из 768-и входных признаков - цифровых эмбеддингов. Дальше можно проводить с ними привычные операции, обучать модели на них и т. п."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87e312-704d-445d-90a9-e6c045ac93c3",
   "metadata": {},
   "source": [
    "### Простая логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de4f24-8c73-47e9-81c7-84aeeb52b28a",
   "metadata": {},
   "source": [
    "Для простоты разбиения датасета на тренировочную и тестовую часть соединим выходные признаки с целевой переменной (некоторую часть таргета придётся отрезать - количество сэмплов фичей пропорционально размеру батча, и какая-то часть из входных сэмплов осталась не-векторизированной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13d9bf92-ebc1-45ea-809c-367ed921fe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7825, 768)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c27915f2-85ca-430c-a786-ff4c514047c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7825, 4)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76bd7386-339c-4fd5-bf77-a274e2f44282",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combined = np.concatenate([features, tw['toxic'][:len(features)].to_numpy().reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2495c751-7f44-440c-9d60-dd3f3c656872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мешаем датасет, потому что дальше возьмём выборку просто как часть датасета, не рандомно\n",
    "np.random.shuffle(ds_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09d458-402e-4fa8-9249-a3171d30976f",
   "metadata": {},
   "source": [
    "Забежим немного вперёд. Дальше нам придётся исправлять дисбаланс в данных, создавая несколько искусственную выборку. Для её проверки полезно отложить какую-то часть от исходного распределения, чем ниже мы и займёмся.\n",
    "\n",
    "Определим часть датасета под отложенную выборку, пусть это будет 8% от неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d26153ab-6d22-4b2e-9f2f-13d8030d5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_num = round(len(ds_combined)*0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb8ea294-036c-4f0f-97c3-3ddc665f11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_validation = ds_combined[0:valid_num]\n",
    "ds_combined = ds_combined[valid_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "02f59b2f-1e66-4133-9a62-685ec7ec9e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626, 769)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d120b5b-a11d-4721-9fe2-08258a0ec703",
   "metadata": {},
   "source": [
    "Какова теперь размерность входных признаков и объединённого датасета для обучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d385bce3-e5ce-48e7-81ea-221cebd339f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7825, 768)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e96fa4a4-ccce-4afb-a8a2-06438d9b3ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7199, 769)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06239b-20fc-4b90-ac34-1b9877fce37e",
   "metadata": {},
   "source": [
    "Всё правильно, добавился один \"столбец\" с таргетом, убавились \"строки\" под отложенную выборку.<br>\n",
    "\n",
    "...а дальше мы столкнулись с проблемой в данных, которую нужно было исправлять. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d0314-a684-447c-bdbf-2e14f22cfdef",
   "metadata": {},
   "source": [
    "#### Несбалансированность датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6dd8d-c089-4f10-aa9c-a11dcfec2c1c",
   "metadata": {},
   "source": [
    "Дело в том, что в датасете наблюдается сильный дисбаланс классов - отметок \"1\" в семь раз меньше, чем отметок \"0\".<br>\n",
    "В результате модель сильно приучается выдавать прогнозы мажорного класса - метрика accuracy при этом может быть очень высокой, но в реальности число ответов false positive \\ false negative (в зависимости от того, какой именно класс мажорный, то есть превалирующий) будет очень велико. Метрики типа F1 тогда будут околонулевыми, а roc-auc - в районе 0.5 (случайное угадывание).\n",
    "\n",
    "\n",
    "Можно снизить влияние фактора разбалансировки, включив параметр <code>stratify</code> при разбиении на тренировочный и тестовый датасеты. Однако, это не помогло, и нам нужно пробовать более \"сильные\" способы борьбы с дисбалансом - такие как добавление количества сэмплов минорного класса (upsampling) и удаление сэмплов мажорного класса (downsampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4fd2b150-5246-45f3-8e04-ab9e157a5c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# yes I'm using histplot here\n",
    "\n",
    "ax = sns.histplot(ds_combined[:, -1], color='darkred')\n",
    "ax.grid(linestyle=\":\", axis='y')\n",
    "ax.set_title('Начальное распределение таргета')\n",
    "ax.set_ylabel('Количество')\n",
    "ax.set_xlabel('Классы');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc9ed0b6-97f8-404a-8f65-fc03bfb9a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_labels_num = len(ds_combined[ds_combined[:, -1] == 0])\n",
    "one_labels_num = len(ds_combined[ds_combined[:, -1] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c7abae9-f24c-4571-b458-483831e1fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сэмплов класса \"0\" в 9.3 раз больше сэмплов класса \"1\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Сэмплов класса \"0\" в {zero_labels_num / one_labels_num:.1f} раз больше сэмплов класса \"1\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da615097-0ef1-4f2c-845b-89d5c12d5cf9",
   "metadata": {},
   "source": [
    "- Исследуем оба подхода."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afedcb-ccc8-426d-89f1-31f098b6ff0e",
   "metadata": {},
   "source": [
    "Делаем сэмплирование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09247ee6-866f-4e18-a400-e61bc87d81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for upsampling\n",
    "ds_resampled = resample(ds_combined[ds_combined[:, -1] == 1], n_samples=zero_labels_num-one_labels_num, random_state=RANDOM_STATE)\n",
    "\n",
    "# for downsampling\n",
    "# ds_resampled = resample(ds_combined[ds_combined[:, -1] == 0], n_samples=one_labels_num,\n",
    "#                         replace=False, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08039d5f-b7e5-4a22-be1a-8a9d6db15c15",
   "metadata": {},
   "source": [
    "Объединяем с исходными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c97a584a-a93c-4271-8436-2d67789a3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling\n",
    "ds_combined = np.vstack((ds_combined, ds_resampled))\n",
    "\n",
    "# downsampling\n",
    "# ds_combined = np.vstack((ds_combined[ds_combined[:, -1] == 1], ds_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a6f2f3a-09ad-4cbe-a2cd-895ebd6cbc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.histplot(ds_combined[:, -1], color='lightblue')\n",
    "ax.grid(linestyle=\":\", axis='y')\n",
    "ax.set_title('Распределение таргета после сэмплирования')\n",
    "ax.set_ylabel('Количество')\n",
    "ax.set_xlabel('Классы');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc0cc8-19ba-4027-9fc9-9d1054756462",
   "metadata": {},
   "source": [
    "- Забегая вперёд - upsampling показал значительно лучшие результаты, но есть нюансы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7ab0cffa-9092-4a58-999b-163d8966e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ds_combined[:, :-1], ds_combined[:, -1], stratify=ds_combined[:, -1],\n",
    "    test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d512374-3792-4bf6-8d1c-40332897b632",
   "metadata": {},
   "source": [
    "Проверим размерности получившихся датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc20d3bb-2e2a-4af0-ad68-8ad6749f8fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности тренировочных датасетов с фичами и таргетом: 💪 (11044, 768) | (11044,)\n",
      "Размерности тестовых датасетов с фичами и таргетом: ✅ (1950, 768) | (1950,)\n"
     ]
    }
   ],
   "source": [
    "print('Размерности тренировочных датасетов с фичами и таргетом:', '💪', X_train.shape, '|', y_train.shape)\n",
    "print('Размерности тестовых датасетов с фичами и таргетом:', '✅', X_test.shape, '|', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f5185-5102-4f27-ac22-d6ab10e0eb48",
   "metadata": {},
   "source": [
    "#### Логрег с исправленным дисбалансом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41fa9712-9a6a-4cac-b364-3e46e1f8a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96       975\n",
      "         1.0       0.93      0.99      0.96       975\n",
      "\n",
      "    accuracy                           0.96      1950\n",
      "   macro avg       0.96      0.96      0.96      1950\n",
      "weighted avg       0.96      0.96      0.96      1950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "preds = logreg.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ce955-505b-41cf-a8f9-293b3217ea5c",
   "metadata": {},
   "source": [
    "Удовлетворяет ли ТЗ метрика F1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "70cd3976-5422-4bf6-99fe-ab79b5617d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9587680079483358\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b58a4261-7e6e-4532-87d4-3baca6399532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Условия ТЗ выполнены! Метрика F1 больше 0.75\n"
     ]
    }
   ],
   "source": [
    "if f1_score(y_test, preds) < MIN_METRIC:\n",
    "    print('Метрика F1 меньше требуемой')\n",
    "else:\n",
    "    print('Условия ТЗ выполнены! Метрика F1 больше', MIN_METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de460b53-f8c6-4845-993a-8bd71a231045",
   "metadata": {},
   "source": [
    "##### Проверка на отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2eb9bf80-6ec3-4f8a-a350-6d3350b26956",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid_ = logreg.predict(ds_validation[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e23c88e3-41d0-4921-b772-0d2744f56dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ds_validation[:, -1], preds_valid_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1c1910d3-a2b3-4901-89bc-cb446d3feaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.91      0.94       551\n",
      "         1.0       0.56      0.81      0.67        75\n",
      "\n",
      "    accuracy                           0.90       626\n",
      "   macro avg       0.77      0.86      0.80       626\n",
      "weighted avg       0.92      0.90      0.91       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ds_validation[:, -1], preds_valid_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b73a8f-3209-4399-95ba-d922efd64924",
   "metadata": {},
   "source": [
    "> Простая логистическая регрессия не даёт нужного результата по метрике, если ориентироваться на отложенную выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1372685-afbb-485b-abfb-14283436613f",
   "metadata": {},
   "source": [
    "### Деревья на градиентном бустинге"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4148ac-6611-4f86-b9a3-bdf5bf303a2c",
   "metadata": {},
   "source": [
    "В этом классе моделей мы будем использовать Light Gradient Boosted Machine (LightGBM). Параметры зададим произвольно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "66e78932-0058-4039-af58-04581c8eeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_estimators = 150, max_depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "406c0d78-62dd-450d-af5a-dd1a8855273f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5522, number of negative: 5522\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 11044, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(max_depth=10, n_estimators=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(max_depth=10, n_estimators=150)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(max_depth=10, n_estimators=150)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "67e89804-0c73-4fed-bef5-50a99d08c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e644f775-f850-4cd1-b6e9-52564b1e58a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913573970513472"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "094d266c-b609-4986-8c5c-0a9bef345b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       975\n",
      "         1.0       0.98      1.00      0.99       975\n",
      "\n",
      "    accuracy                           0.99      1950\n",
      "   macro avg       0.99      0.99      0.99      1950\n",
      "weighted avg       0.99      0.99      0.99      1950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411c80d-8db3-4831-8394-32f9133723e7",
   "metadata": {},
   "source": [
    "Удовлетворяет ли ТЗ метрика F1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c7df047-3969-495c-80d6-7483df90f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Условия ТЗ выполнены! Метрика F1 больше 0.75\n"
     ]
    }
   ],
   "source": [
    "if f1_score(y_test, preds) < MIN_METRIC:\n",
    "    print('Метрика F1 меньше требуемой')\n",
    "else:\n",
    "    print('Условия ТЗ выполнены! Метрика F1 больше', MIN_METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0b25b-7493-4654-a953-29059a211a2d",
   "metadata": {},
   "source": [
    "> Модель на градиентном бустинге даже без подбора параметров показывает очень высокий результат, подходящий нам по ТЗ.\n",
    ">\n",
    "> <u>Однако не стоит доверять этому результату, потому что он получен на части синтетических данных</u>. Более правильным подходом всё же видится downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e653e55a-fce6-4add-a6f1-22d1dbf8ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = lgbm.predict(ds_validation[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "415017c8-a64d-4287-baee-0fae3c9d542e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ds_validation[:, -1], preds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8cc1bad6-75cb-49bf-939e-2b1c52fd716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96       551\n",
      "         1.0       0.72      0.59      0.65        75\n",
      "\n",
      "    accuracy                           0.92       626\n",
      "   macro avg       0.83      0.78      0.80       626\n",
      "weighted avg       0.92      0.92      0.92       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ds_validation[:, -1], preds_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b57074-2e05-4b4e-b53c-6ac8ba092afc",
   "metadata": {},
   "source": [
    "### Сводная таблица с метрикой F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea712611-d84c-4ebf-bfb0-fbece4f59b23",
   "metadata": {},
   "source": [
    "| Модель, размер датасета | Тестовые сэмплированные данные | Отложенная выборка вне сэмплирования |\n",
    "|:---------|:--------:|:--------------------------------------------:|\n",
    "|Logistic Regression, 1% |  0.96  | 0.71|\n",
    "|Logistic Regression, 5% |  0.95  |  0.61 |\n",
    "|Logistic Regression, 20%| 0.9 | 0.6 |\n",
    "|Logistic Regression, 95%|0.89|0.63|\n",
    "|LGBMClassifier, 1% |0.98|0.45|\n",
    "|LGBMClassifier, 5% |0.99|0.65|\n",
    "|LGBMClassifier, 20% |  0.98   |  0.75  |\n",
    "|LGBMClassifier, 95% |0.95|0.71|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81afbb0-45c7-498c-a185-770f45b24ca5",
   "metadata": {},
   "source": [
    "### Вывод по применению классических моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4b3d0-7b86-4c5f-8a3b-7e5e97d43581",
   "metadata": {},
   "source": [
    "Не стоит слишком доверять около-идеальному значению метрики на сэмплированных данных, всё же это несколько искусственные выборки. На реальных данных мы скорее увидим примерно такие значения F1, как в расчётах с отложенной выборкой. Нас, как дата-сайентистов, подобные показатели устраивать не могут. Можно, конечно, устроить подбор параметров по GridSearch и выбить нужные значения метрики качества...\n",
    "\n",
    "Но что, если посмотреть, какие результаты покажут специально предназначенные для работы с текстом нейро-модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff990b8f-3e7d-4c5c-84f8-ae6f17d6eea2",
   "metadata": {},
   "source": [
    "### Использование модели BERT для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a179432e-8aed-4bf2-be46-95723c0e1648",
   "metadata": {},
   "source": [
    "Одна из главных непонятностей этого проекта заключается в том, а зачем мы вообще возимся с эмбеддингами, чтобы применить их в \"классических\" моделях машинного обучения. В реальной жизни специалисты скорее возьмут одну из существующих свободно-распространяемых моделей глубокого обучения, до-обучат её на собранном под их задачу датасете и будут успешно применять. Однако в нашей теории нет ничего такого, и, например, архитектура трансформеров - одна из основ в NLP - совсем не затронута."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b04669-eb90-47ee-9f54-6bac6770da78",
   "metadata": {},
   "source": [
    "Чтобы извлечь хоть какую-то пользу из этого проекта, мы решили включить режим самообучения, и попробовать решить стоящую задачу непосредственно на BERT'e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4396d-ec42-44f1-930e-08803775955f",
   "metadata": {},
   "source": [
    "Это довольно сложная задача для начинающего data scientist'а без опыта работы (и даже учёбы) с нейронками, поэтому основную часть кода для обучения и валидации мы взяли здесь: https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model, исправив вылезшие ошибки и поменяв метрику качества."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4f258-5480-4f10-b0cc-c30ab5c4f169",
   "metadata": {},
   "source": [
    "#### Исправление дисбаланса. Смещённый downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698178f0-bb2d-4767-ba51-7498d190516e",
   "metadata": {},
   "source": [
    "На этот раз применим немного более хитрую схему - мы не будем уравнивать количество сэмплов обоих классов через downsampling, а возьмём чуть бóльшую долю превалирующего класса. Наша интуиция в том, что такое выравнивание более точно воспроизведёт реальную ситуацию на реальных данных, ведь если модель обучится на данных с равным количеством классов, а в продакшне будет получать сильный дисбаланс, она вряд ли покажет себя хорошим прогнозатором. Мы назвали эту схему \"выравнивание со смещением\".<br>\n",
    "\"Смещение\" задаётся параметром BALANCE_SHIFT. Например, если этот параметр равен 1.5, то после downsampling количество сэмплов мажорного класса будет в 1.5 больше количества сэмплов минорного класса.\n",
    "\n",
    "N. B. видится логичным задавать смещение как долю от реального коэффициента дисбаланса, но чтобы соотношение количества сэмплов разных классов было, к примеру, не больше 1.5-1.9, но ниже мы реализуем просто \"константный\" подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b5e2cf26-ea04-4cce-ade9-1c66a91f94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_downsampling(data: pd.DataFrame, class_col: str, \n",
    "                         balance_shift: float, shuffle: bool=True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Downsampling for binary classification tasks with number of major class samples = N * number of \n",
    "    minor class samples, where N is 'balance_shift' variable.\n",
    "    \n",
    "    'class_col' - name of column with classes labels;\n",
    "    'shuffle' - if False, resulting dataset will not be shuffled, making it look like\n",
    "                it's sorted by column 'class_col';\n",
    "                \n",
    "    '''\n",
    "    \n",
    "    one_labels_num = (data[class_col] == 1).sum()\n",
    "    zero_labels_num = (data[class_col] == 0).sum()\n",
    "    \n",
    "    imbalance_coef = zero_labels_num / one_labels_num\n",
    "    \n",
    "    if zero_labels_num < one_labels_num:\n",
    "        major_class = 1\n",
    "        minor_labels_num = zero_labels_num\n",
    "    else:\n",
    "        major_class = 0\n",
    "        minor_labels_num = one_labels_num\n",
    "\n",
    "    num_samples = round(balance_shift*minor_labels_num)\n",
    "    if num_samples > len(data[data[class_col] == major_class]):\n",
    "        return(f'Too few data to resample with balance_shift = {balance_shift}')\n",
    "    \n",
    "    resampled = resample(data[data[class_col] == major_class], n_samples=num_samples,\n",
    "                            replace=False, random_state=2042)\n",
    "    \n",
    "    data = pd.concat((data[data[class_col] == 1 - major_class], resampled), axis=0)\n",
    "\n",
    "    # at this moment we have first part of DS of minor class, second part - major\n",
    "    # it's better to shuffle these data by default\n",
    "    if shuffle:\n",
    "        data = data.sample(frac = 1, replace=False)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af4beb-d928-436e-a376-5a7ec509fdfd",
   "metadata": {},
   "source": [
    "Производим downsampling и выводим некоторые цифры для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3ec04a53-c1f5-45f7-83b6-cad9882cebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = shifted_downsampling(tw, 'toxic', 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d9183a99-26a6-4c74-8ba0-dc337b68c3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    1166\n",
       "1     777\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "071b56fc-4d64-4f1c-8d03-6dcae44681e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер после downsampling со смещением: (1943, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Размер после downsampling со смещением:', tw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e8033-e920-4160-ad85-31af62858d3a",
   "metadata": {},
   "source": [
    "#### Подготовка данных под формат, требуемый моделью"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6c407-d42f-46cf-9e9c-e4a7d6d9b917",
   "metadata": {},
   "source": [
    "Для использования модели BERT наши данные должны иметь формат тензоров, а пока у нас отдельные числа (отметка токсичности) и списки с цифрами (исходные комментарии в виде токенов и маска внимания). Исправим это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6fc94a51-0dee-40c9-91b4-69bce5b7d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokens = torch.LongTensor(np.vstack(tw['tokens']))\n",
    "tensor_masks = torch.LongTensor(np.vstack(tw['mask']))\n",
    "tensor_labels = torch.LongTensor(np.array(tw['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "880b9697-6b39-400b-ae88-3a68e05728bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.Size([1943]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_labels.dtype, tensor_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "09ec3529-f0e7-464a-bc8b-83e98145850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1943, 512])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e64e3d-28b7-42d7-850b-646308cec214",
   "metadata": {},
   "source": [
    "Создадим специальный PyTorch датасет, объединяя данные и разметку классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3d479242-1274-4a08-9f3c-f3e5667fb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(tensor_tokens, tensor_masks, tensor_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd23a09-32ba-40fe-8082-b2660bc6fd2d",
   "metadata": {},
   "source": [
    "Разбиваем на тренировку и тест, как обычно. Только теперь используем метод не из Sklearn, а из PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f032ad61-d293-4f11-8b94-4f18a3ec71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сэмплов в тренировочной части: 1652\n",
      "Количество сэмплов в тестовой части: 291\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [1 - TEST_SIZE, TEST_SIZE])\n",
    "\n",
    "print('Количество сэмплов в тренировочной части:', len(train_dataset))\n",
    "print('Количество сэмплов в тестовой части:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5f5de-cbb1-4ea5-aead-08e601200d47",
   "metadata": {},
   "source": [
    "Дальше создаются загрузчики данных - экземпляры специального класса DataLoader, которые будут подгружать в модель данные частями = батчами, чтобы наша память не взорвалась. Размер такого куска задаём в переменной batch_size.\n",
    "\n",
    "N. B. Батчи длиннее восьми наше железо не выдерживает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "565a4034-fb63-4727-9c51-c5ab93635fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            shuffle=False,\n",
    "            # sampler = RandomSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ff081-0732-4d04-9880-0454287fd39e",
   "metadata": {},
   "source": [
    "#### Инициализация модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089aa61-bd6a-4d3a-a6d3-f74a12fe0914",
   "metadata": {},
   "source": [
    "Дальше инициализируем модель <code>BertForSequenceClassification</code>, предназначенную ровно под наш тип задач"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1c9923d9-d13c-4322-8fb7-3802a239b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model_classify = transformers.BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# переносим модель на доступный 'девайс'\n",
    "model_classify = model_classify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7f04cbc-2d6b-4e3f-a746-19929b6dd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb258e-90d0-4d80-98d6-386718700a92",
   "metadata": {},
   "source": [
    "#### Основная часть - тренировочный и валидационный цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c14f3ce0-5a7d-4658-b34f-92379ea2c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4ae6b9f6-82fa-417c-8ddf-e1fa6daf6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating custom F1 (harmonic mean of precision and recall)\n",
    "\n",
    "def f1_custom(tp: int, fp: int, fn: int) -> float:\n",
    "    '''\n",
    "    Compute F1 score using numbers of True, False positive and False negative.\n",
    "     \n",
    "    '''\n",
    "    precision = tp / (tp + fp + EPS)\n",
    "    recall = tp / (tp + fn + EPS)\n",
    "    return 2*precision*recall / (precision + recall + EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1e85bf0e-7d15-4cb5-83da-dbe097fb0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = tm.time()\n",
    "\n",
    "if not(SKIP_BERT):\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "    \n",
    "        print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "    \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = tm.time()\n",
    "    \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "    \n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model_classify.train()\n",
    "    \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "            # Progress update every ... batches.\n",
    "            if step % 30 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(tm.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,} of {:>4,}.   Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "    \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "    \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            \n",
    "            # model_classify.zero_grad()\n",
    "            # в других источниках оптимайзер обнуляют. будет ли разница?\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our usage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "                      \n",
    "            outputs = model_classify(input_ids=b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask,\n",
    "                                 labels = b_labels\n",
    "                                 )\n",
    "            #this loss worked ok, we've tried another one below\n",
    "            loss = outputs[0]\n",
    "    \n",
    "            pred = torch.sigmoid(outputs[1][:, 1]).detach().cpu()\n",
    "            tru = torch.tensor(b_labels.to('cpu'))\n",
    "            # loss = loss_fn(pred, torch.FloatTensor(tru*1.0))\n",
    "         \n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            # loss.requires_grad = True\n",
    "            loss.backward()\n",
    "    \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model_classify.parameters(), 1.0)\n",
    "    \n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        loss_plot.append(avg_train_loss)\n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(tm.time() - t0)\n",
    "    \n",
    "        print(\"\\n   Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"   Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "    \n",
    "        print(\"\\nRunning Validation...\")\n",
    "    \n",
    "        t0 = tm.time()\n",
    "    \n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model_classify.eval()\n",
    "    \n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        TP, FP, FN = 0, 0, 0\n",
    "    \n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(test_dataloader):\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "    \n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                outputs = model_classify(\n",
    "                                input_ids = b_input_ids, \n",
    "                                token_type_ids = None, \n",
    "                                attention_mask = b_input_mask,\n",
    "                                labels = b_labels\n",
    "                                )\n",
    "    \n",
    "            logits = outputs[0]\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += logits.item()\n",
    "    \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu()\n",
    "            label_ids = b_labels.to('cpu')\n",
    "            \n",
    "            y_pred = torch.max(outputs[1].cpu(), axis=1)[1]\n",
    "            y_true = label_ids\n",
    "    \n",
    "            tp = ((y_pred == 1) & (y_true == 1)).sum().numpy()\n",
    "            fp = ((y_pred == 1) & (y_true == 0)).sum().numpy()\n",
    "            fn = ((y_pred == 0) & (y_true == 1)).sum().numpy()\n",
    "    \n",
    "            TP += tp\n",
    "            FP += fp\n",
    "            FN += fn\n",
    "    \n",
    "            if step % 10 == 0 and step !=0:\n",
    "                print('  Batch {:>4,}  of  {:>4,}'.format(step, len(test_dataloader)))\n",
    "    \n",
    "       \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(tm.time() - t0)\n",
    "    \n",
    "        metric_f1 = f1_custom(TP, FP, FN)\n",
    "    \n",
    "        print(\"\\n   F1-score: {0:.2f}\".format(metric_f1))\n",
    "        print(\"   Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"   Validation took: {:}\".format(validation_time))\n",
    "    \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time,\n",
    "                'F1-score': metric_f1\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(tm.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "625e1c10-2f73-40ff-bcda-a9801e8b3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'trained_model_x'+str(DATASET_SPLIT)+'.pt')\n",
    "#model.save_pretrained('trained_model_x') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab98cbc-01e0-4cc4-bdb7-d41b4b70ea09",
   "metadata": {},
   "source": [
    "#### Промежуточные итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf24659-fb5d-4a2a-9a37-aa2509b6bbcc",
   "metadata": {},
   "source": [
    "На несбалансированном датасете размером в 0.01 (1%) от оригинала BERT показывает метрику F1 = 0.21 на валидационной части. Уже чуть лучше, классические модели показывали 0 (ноль). Но главное другое - <u>спустя бессчётное количество итераций нам наконец-то удалось заставить работать трен. и валидационные циклы</u>. Теперь можно ставить эксперименты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d56ed2-e153-4054-8816-707a7adf1339",
   "metadata": {},
   "source": [
    "#### Важное примечание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da431c-2ece-414d-ad22-2c666175ee89",
   "metadata": {},
   "source": [
    "Наш второй цикл - после тренировочного - называется <i>валидационным</i>, но по сути мы используем разбиение датасета только на две выборки - тренировочную и тестовую. Можно выделять и третью выборку, и потом проводить её через такой же цикл, как валидационный, и показатели качества на ней учитывать как финальные, но это увеличит количество кода, и к тому же мы не проводим тюнинг гипер-параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a2043-8a06-4d1d-9cf4-80452c0885d6",
   "metadata": {},
   "source": [
    "#### Таблица с результатами классификации c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab9635-1421-4272-9e17-f0022301d8f4",
   "metadata": {},
   "source": [
    "Ниже приведено максимальное значение метрики F1 для различного объёма данных из датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a2708-4b4c-4009-883b-d3ecc258c9bb",
   "metadata": {},
   "source": [
    "| Размер датасета | Исходные данные | Данные без дисбаланса |Баланс+оставление стоп-слов| Время обучения, ч:мм |\n",
    "|:---------|:--------:|:---------:|:---------:|:---------:|\n",
    "|1% |  0.21 |0.49|0.58| 0:09 |\n",
    "|5% | 0.05   |0.57||1:25 |\n",
    "|20%| 0.19 | 0.45 |0.55|3:56|\n",
    "|95%|||0.54|4:43|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d2b7c-6dc4-4c65-8f17-d2a0a75586a2",
   "metadata": {},
   "source": [
    "#### Вывод по использованию BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef8459-87d7-47e4-bbea-9a49693a95d5",
   "metadata": {},
   "source": [
    "Видим, что метрика в данных без исправленного дисбаланса хуже, и ещё и нестабильнее. Поэтому следует исправлять дисбаланс.\n",
    "<br>\n",
    "Интересно также, что <i>неудаление</i> стоп-слов улучшает метрику почти на 10%! BERT продвинутый, подобная чистка ему не нужна.<br><br>\n",
    "Средний лосс на тренировке составляет около 0.9 и практчески не изменяется от эпохи к эпохе, как и метрика качества, то есть проводить больше 2-х эпох не имеет смысла (1 эпоха длилась около 3,5 часов, а 3 эпохи - 11 часов).\n",
    "<br><br>\n",
    "Плохо то, что <u>на том же объёме данных мы не получаем качества лучше, чем у классических моделей</u>, при том, что работают они на порядки быстрее. <b>Метрика F1 не достигла даже 0.6</b>. Возможно, имеющийся датасет слишком мал, чтобы LLM могли показать себя в полную мощь - в максимуме модель обучалась на 34628 сэмплах, а тестировалась на 6110.<br>\n",
    "Либо мы что-то делаем не так."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd5a333-c5d6-4b9d-a00e-0e24b6aa2cca",
   "metadata": {},
   "source": [
    "### Предобученная модель RoBERTa toxicity classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5133296-f3ac-49a0-8a19-d99ee5d41810",
   "metadata": {},
   "source": [
    "Возможно, нас выручит модель, обученная ровно под нашу задачу. Это классификатор токсичности сообщений, основанный на модели RoBERTa (один из многих улучшенных вариантов BERT).<br>\n",
    "С моделью можно ознакомиться здесь: https://huggingface.co/s-nlp/roberta_toxicity_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "adcf7029-2f9c-436a-aa46-5be09a866886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('s-nlp/roberta_toxicity_classifier')\n",
    "model_roberta = transformers.AutoModelForSequenceClassification.from_pretrained(\"s-nlp/roberta_toxicity_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d4d27-ee4f-4682-9a96-a84ebf7f3171",
   "metadata": {},
   "source": [
    "> Мы уже исправляли дисбаланс в датафрейме <code>tw</code> (см. раздел с BERT \"Исправление дисбаланса\"), поэтому делать это заново не нужно. Убедимся только, что всё точно сохранилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3d04ba4b-7a52-4b8d-ba1d-e2bdab7b46b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    1166\n",
       "1     777\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9d549-3fab-4795-a091-329286db9866",
   "metadata": {},
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324562fd-baad-4d96-8f58-007a1e51429c",
   "metadata": {},
   "source": [
    "В первоисточнике предлагается не тот же токенизитор, что мы использовали для BERT. Он выдаёт другие токены, нам нужно заново токенизировать тексты.\n",
    "\n",
    "Если мы уже проводили процедуры токенизации, добавления маски внимания и паддинга, считаем полученный файл с результатами, что существенно ускорит исследовательский процесс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a16cd47c-6612-4140-8ee7-c41f17ce6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_tokenized_masked_padded = False\n",
    "\n",
    "if os.path.exists('rb_tokenized_masked_padded_x'+str(DATASET_SPLIT)+'.npy'):\n",
    "    rb_tokenized_masked_padded = True\n",
    "\n",
    "    # saving to .csv messes with lists of nums, so we use numpy->pd.DF\n",
    "    roberta_df = np.load('rb_tokenized_masked_padded_x'+str(DATASET_SPLIT)+'.npy', allow_pickle=True)\n",
    "    roberta_df = pd.DataFrame(roberta_df)\n",
    "    roberta_df.rename(columns={0:'text', 1: 'tokens', 2:'mask', 3:'label'}, inplace=True)\n",
    "    roberta_df['label'] = roberta_df['label'].astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be7bb0-0e26-4152-ad91-97172ead837b",
   "metadata": {},
   "source": [
    "Сделаем отдельный датафрейм для этой модели.<br>\n",
    "\n",
    "В процессе исследования мы выяснили, что токенизаторы умеют сами достраивать паддинг и делать маску внимания. Воспользуемся этим для ускорения и чистоты кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9689b4eb-f0a5-4d14-a8f6-ed20a4e824d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not(rb_tokenized_masked_padded):\n",
    "    \n",
    "    roberta_df = pd.DataFrame(index=range(len(tw)), columns=['text', 'tokens', 'mask', 'label'])\n",
    "    \n",
    "    for line, text in enumerate(tw['text']):\n",
    "\n",
    "        tkn = tokenizer(text, max_length=512, padding='max_length', padding_side='right', truncation=True)\n",
    "        roberta_df['tokens'].iloc[line] = tkn['input_ids']\n",
    "        roberta_df['mask'].iloc[line] = tkn['attention_mask']\n",
    "        roberta_df['label'].iloc[line] = tw['toxic'].iloc[line]\n",
    "        roberta_df['text'].iloc[line] = tw['text'].iloc[line]\n",
    "       \n",
    "\n",
    "    # saving our results to not calculate all of those from scratch\n",
    "    try:\n",
    "        np.save('rb_tokenized_masked_padded_x'+str(DATASET_SPLIT)+'.npy', roberta_df.to_numpy())\n",
    "    except:\n",
    "        print('Не получилось записать данные обработки текстов. Возможно, нет разрешения на запись в данном месте')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2fd184ec-90f0-453e-a9df-66565a9bf8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1943 entries, 0 to 1942\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1943 non-null   object\n",
      " 1   tokens  1943 non-null   object\n",
      " 2   mask    1943 non-null   object\n",
      " 3   label   1943 non-null   uint8 \n",
      "dtypes: object(3), uint8(1)\n",
      "memory usage: 47.6+ KB\n"
     ]
    }
   ],
   "source": [
    "roberta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38573ff6-8693-434d-a7ac-0ec171baf4b4",
   "metadata": {},
   "source": [
    "- Все данные для модели нужно преобразовать в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1f56c4de-f493-4603-bbc1-907b657e6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokens = torch.LongTensor(np.vstack(roberta_df['tokens']))\n",
    "tensor_masks = torch.LongTensor(np.vstack(roberta_df['mask']))\n",
    "tensor_labels = torch.LongTensor(np.array(roberta_df['label'].astype('int')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1970fc-3fb7-4ddd-ada9-81e64e24994e",
   "metadata": {},
   "source": [
    "- Создадим специальный PyTorch датасет, объединяя данные и разметку классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1bf685ef-2f03-41b6-9cbd-218158353758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the all inputs into a TensorDataset.\n",
    "dataset = TensorDataset(tensor_tokens, tensor_masks, tensor_labels)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# we don't need to train and validate this model, so all the data is feeding now\n",
    "all_data_dataloader = DataLoader(\n",
    "            dataset,\n",
    "            sampler = RandomSampler(dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dec5f55a-5c3b-4a6e-ae84-99553f2f8186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sending our model to device\n",
    "model_roberta.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3afc1-4abc-41f4-92b5-23bb80053233",
   "metadata": {},
   "source": [
    "Для получения предсказаний по всему датасету, а затем замера метрик на них, сделаем такой же цикл, как и валидационный цикл для BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "35120c75-6291-47ea-8c31-f06fe27e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN = 0, 0, 0\n",
    "total_eval_loss = 0\n",
    "t0 = tm.time()\n",
    "\n",
    "if not(SKIP_ROBERTA):\n",
    "    print('\\n======== Считаем качество на предобученной модели RoBERTa ========')\n",
    "    \n",
    "    for step, batch in enumerate(all_data_dataloader):\n",
    "    \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model_roberta(\n",
    "                            input_ids = b_input_ids, \n",
    "                            token_type_ids = None, \n",
    "                            attention_mask = b_input_mask,\n",
    "                            labels = b_labels\n",
    "                            )\n",
    "    \n",
    "        logits = outputs[0]\n",
    "        # we don't actually need a loss here, but let it stays.\n",
    "        total_eval_loss += logits.item()\n",
    "       \n",
    "        # pulling out predictions from logits\n",
    "        y_pred = torch.max(outputs[1].cpu(), axis=1)[1]\n",
    "        # true labels is just incoming labels\n",
    "        y_true = batch[2]\n",
    "    \n",
    "        # numbers of true\\false positive\\negatives in the batch\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum().numpy()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum().numpy()\n",
    "        fn = ((y_pred == 0) & (y_true == 1)).sum().numpy()\n",
    "    \n",
    "        # total numbers of true\\false positive\\negatives\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        FN += fn\n",
    "    \n",
    "        if step % 25 == 0 and step !=0:\n",
    "            print('  Batch {:>4,}  of  {:>4,}'.format(step, len(all_data_dataloader)))\n",
    "    \n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(all_data_dataloader)\n",
    "    \n",
    "    # Measure how long the run took.\n",
    "    validation_time = format_time(tm.time() - t0)\n",
    "    \n",
    "    metric_f1 = f1_custom(TP, FP, FN)\n",
    "    \n",
    "    print(f'\\nМетрика F1 на датасете размером в {100*DATASET_SPLIT}% = {metric_f1:.03f}')\n",
    "    print(f'Метрика recall на датасете размером в {100*DATASET_SPLIT}% = {TP/(TP+FN):.03f}')\n",
    "    print(f'Метрика precision на датасете размером в {100*DATASET_SPLIT}% = {TP/(TP+FP):.03f}\\n')\n",
    "    print(\"  Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Counting took: {:}\".format(validation_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca2985-9561-465b-a7a0-862a3abf921f",
   "metadata": {},
   "source": [
    "#### Предварительные результаты на модели RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230bd2a-8544-47cb-9556-0ccf2a32349c",
   "metadata": {},
   "source": [
    "<code>\n",
    "Метрика F1 на датасете размером в 1.0% = 0.849\n",
    "Метрика recall на датасете размером в 1.0% = 0.837\n",
    "Метрика precision на датасете размером в 1.0% = 0.861\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0ed49-5a07-4d3b-9b0e-4446ac89791e",
   "metadata": {},
   "source": [
    "Ух! Размер датасета слишком мал, но выглядит очень многообещающе! И, кажется, в отличие от всех предыдущих подходов, эта модель умеет хорошо предсказывать истинно положительные случаи (высокий precision). Попробуем больший размер."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83ccc0-558c-4e0b-81fe-1f3185aef1e4",
   "metadata": {},
   "source": [
    "#### Таблица с результатами классификации c RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41eda5e-c325-478c-9b5c-8aa850325acb",
   "metadata": {},
   "source": [
    "Ниже приведено максимальное значение метрики F1 для различного объёма данных из датасета. Был исправлен дисбаланс классов со смещением, удаление стоп-слов не производилось."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275d57f-65ac-4023-979e-440b77f9c7d3",
   "metadata": {},
   "source": [
    "| Размер датасета | recall | precision |F1|\n",
    "|:---------|:--------:|:---------:|:---------:|\n",
    "|1% | 0.837 |0.861|0.849|\n",
    "|5% | 0.830 |0.973|0.898|\n",
    "|95%| 0.834 |0.974|0.899|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69028e71-fc3e-4790-b14e-6373f7b4d43b",
   "metadata": {},
   "source": [
    "### Проверка модели на произвольном тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d89196-5566-4662-aea7-0613cabb9576",
   "metadata": {},
   "source": [
    "В коде ниже, записывая свой текст в переменную <code>text</code>, можно протестировать модель на произвольных предложениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bbdb7dba-0cda-4dc9-b4e5-cce935da8060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is:\n",
      " «Purgatory's kind of like the in-betweeny one. You weren't really shit, but you weren't all that great either. Like Tottenham» \n",
      "\n",
      "Decision of model:\n",
      "  sentence is toxic\n"
     ]
    }
   ],
   "source": [
    "text = \"Purgatory's kind of like the in-betweeny one. You weren't really shit, but you weren't all that great either. Like Tottenham\"\n",
    "\n",
    "model_roberta.to('cpu')\n",
    "text_tokenized = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pred = model_roberta(text_tokenized).logits.argmax().item()\n",
    "\n",
    "print(f'Sentence is:\\n «{text}» \\n')\n",
    "print('Decision of model:')\n",
    "if pred:\n",
    "    print('  sentence is toxic')\n",
    "else:\n",
    "    print('  sentence is neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4d781-21e4-41fc-85e4-b8d847d27559",
   "metadata": {},
   "source": [
    "### Выводы по проекту"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d3b9c-4c23-4735-9d3e-aed0821a13f4",
   "metadata": {},
   "source": [
    "В этом проекте мы изучили некоторые способы работы с текстом в машинном обучении.<br><br>\n",
    "Тексты предварительно обрабатывались: проводилась очистка текста от стоп-слов (не для всех моделей), с помощью регулярных выражений мы убирали даты и числа, пунктуацию. Очищенные тексты токенизировались - слова заменялись числами. Затем \"вручную\" производился паддинг (выравнивание длины всех комментариев, что необходимо для работы моделей в batch-режиме) и добавление маски внимания. После этого мы пропускали все токены через модель <code>BERT</code>, забирая получившиеся эмбеддинги - таким образом мы провели <i>векторизацию</i> текстов.<br><br>\n",
    "Далее, используя эмбеддинги в качестве входных признаков (фичей) мы проверили классические подходы и убедились, что они работают не очень стабильно по качеству на несбалансированных датасетах, но зато очень быстро. Мы получили минимально необходимую метрику качества F1 (0.75) на модели <code>LGBMClassifier</code>, и могли бы ещё пытаться улучшать её, используя разные способы исправления дисбаланса классов и подбирая параметры модели по сетке, но решили шагнуть в другую сторону и исследовать нейронные сети.<br><br>\n",
    "Сперва мы попытались использовать тот же BERT, которым векторизовали тексты ранее. Мы адаптировали код тренировочного и валидационного цикла под нашу задачу и пытались дообучать модель. Но метрика F1 не превысила 0.6, а длились эксперименты очень долго. Подробнее об этом см. в разделе «Вывод по использованию BERT».<br><br>\n",
    "Затем мы пошли более простым путём: стали искать предобученную для похожей на нашу задачу модель. Первая же проверенная в деле модель <code>RoBERTa toxicity classifier</code> от российских ML-инженеров дала очень хорошие результаты: не только F1 на максимуме равнялась 0.899, но и метрики precision и recall составляли, соответственно, 0.974 и 0.834 - чем не могли похвастаться все предыдущие модели, которые намного хуже предсказывали класс 1 (токсичные комментарии), и в результате имели precision сильно меньше recall.<br><u><b>На этом задачу проекта мы посчитали выполненной.</b></u>\n",
    "<br><br>\n",
    "Что можно было сделать ещё.<br>\n",
    "Большие модели тоже можно по-разному подкручивать, как мы это делали в классическом ml, настраивать их параметры, такие как learning rate и прочее. Мы этого практически не коснулись из-за недостатка опыта и ограниченности времени."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342f29b-e841-4b12-b73b-17f79f7346ca",
   "metadata": {},
   "source": [
    "<b>N. B.</b> Строго говоря, задача описывалась как обучить классифицировать комментарии на <u>позитивные</u> и негативные, но в исходном датасете тексты размечены как негативные (токсичные) и <i>не</i> негативные - то есть в этой категории есть и нейтральные, и позитивные. Мы это приняли как недоработку в постановке задачи, но вообще такой момент в реальности сразу бы стоило уточнять у заказчика."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
